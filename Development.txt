https://pro.psychcentral.com/the-five-types-of-avoidance/

ML:
https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/?utm_source=facebook.com

pandas:
https://www.datacamp.com/courses/analyzing-police-activity-with-pandas

https://www.datacamp.com/community/tutorials/joining-dataframes-pandas

https://www.datacamp.com/community/tutorials/pandas-tutorial-dataframe-python

bball nba games:
ttps://www.analyticsvidhya.com/blog/2019/05/scraping-nba-data-analyze-1000-basketball-games-python/?utm_source=facebook.com&utm_medium=social


reading various formats:
https://www.analyticsvidhya.com/blog/2017/03/read-commonly-used-formats-using-python/?utm_source=facebook.com&utm_medium=social


scikit cheat sheet:
https://www.facebook.com/726282547396228/photos/a.757399000951249/2232589283432206/?type=3

1) linear regression

#Import Library
#Import other necessary libraries like pandas, numpy...
from sklearn import linear_model
#Load Train and Test datasets
#Identify feature and response variable(s) and values must be numeric and numpy arrays
x_train=input_variables_values_training_datasets
y_train=target_variables_values_training_datasets
x_test=input_variables_values_test_datasets
# Create linear regression object
linear = linear_model.LinearRegression()
# Train the model using the training sets and check score
linear.fit(x_train, y_train)
linear.score(x_train, y_train)
#Equation coefficient and Intercept
print('Coefficient: \n', linear.coef_)
print('Intercept: \n', linear.intercept_)
#Predict Output
predicted= linear.predict(x_test)

also:
https://medium.com/analytics-vidhya/linear-regression-in-python-from-scratch-24db98184276

https://www.datacamp.com/community/tutorials/tutorial-ridge-lasso-elastic-net

https://www.datacamp.com/courses/introduction-to-linear-modeling-in-python

2) Logistic Regression

#Import Library
from sklearn.linear_model import LogisticRegression
#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset
# Create logistic regression object
model = LogisticRegression()
# Train the model using the training sets and check score
model.fit(X, y)
model.score(X, y)
#Equation coefficient and Intercept
print('Coefficient: \n', model.coef_)
print('Intercept: \n', model.intercept_)
#Predict Output
predicted= model.predict(x_test)

also for logistic regression:
https://medium.com/analytics-vidhya/python-implementation-of-andrew-ngs-machine-learning-course-part-2-1-1a666f049ad6 

https://www.datacamp.com/community/tutorials/understanding-logistic-regression-python

https://medium.com/analytics-vidhya/python-implementation-of-andrew-ngs-machine-learning-course-part-2-2-dceff1a12a12

https://medium.com/analytics-vidhya/a-guide-to-using-logistic-regression-for-digit-recognition-with-python-codes-86aae6da10fe

https://www.analyticsvidhya.com/blog/2017/08/skilltest-logistic-regression/?utm_source=facebook.com

3)  Decision Tree

#Import Library
#Import other necessary libraries like pandas, numpy...
from sklearn import tree
#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset
# Create tree object 
model = tree.DecisionTreeClassifier(criterion='gini') # for classification, here you can change the algorithm as gini or entropy (information gain) by default it is gini  
# model = tree.DecisionTreeRegressor() for regression
# Train the model using the training sets and check score
model.fit(X, y)
model.score(X, y)
#Predict Output
predicted= model.predict(x_test)

https://www.datacamp.com/courses/machine-learning-with-tree-based-models-in-python

4) Support Vector Machine

#Import Library
from sklearn import svm
#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset
# Create SVM classification object 
model = svm.svc() # there is various option associated with it, this is simple for classification. You can refer link, for mo# re detail.
# Train the model using the training sets and check score
model.fit(X, y)
model.score(X, y)
#Predict Output
predicted= model.predict(x_test)

https://www.analyticsvidhya.com/blog/2017/10/svm-skilltest/?utm_source=facebook.com

https://www.datacamp.com/community/tutorials/machine-learning-python

5) Naive Bayes - simple & high performing

#Import Library
from sklearn.naive_bayes import GaussianNB
#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset
# Create SVM classification object model = GaussianNB() # there is other distribution for multinomial classes like Bernoulli Naive Bayes, Refer link
# Train the model using the training sets and check score
model.fit(X, y)
#Predict Output
predicted= model.predict(x_test)

https://www.analyticsvidhya.com/blog/2016/06/bayesian-statistics-beginners-simple-english/?utm_source=facebook.com&utm_medium=social


6) kNN - mostly used for classification but can be used for regression

#Import Library
from sklearn.neighbors import KNeighborsClassifier
#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset
# Create KNeighbors classifier object model 
KNeighborsClassifier(n_neighbors=6) # default value for n_neighbors is 5
# Train the model using the training sets and check score
model.fit(X, y)
#Predict Output
predicted= model.predict(x_test)

also kNN:
https://www.datacamp.com/community/tutorials/k-nearest-neighbor-classification-scikit-learn
https://www.analyticsvidhya.com/blog/2018/08/k-nearest-neighbor-introduction-regression-python/?utm_source=facebook.com


7) k Means - unsupervised

#Import Library
from sklearn.cluster import KMeans
#Assumed you have, X (attributes) for training data set and x_test(attributes) of test_dataset
# Create KNeighbors classifier object model 
k_means = KMeans(n_clusters=3, random_state=0)
# Train the model using the training sets and check score
model.fit(X)
#Predict Output
predicted= model.predict(x_test)

8) Random forest - ensemble of decision trees

#Import Library
from sklearn.ensemble import RandomForestClassifier
#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset
# Create Random Forest object
model= RandomForestClassifier()
# Train the model using the training sets and check score
model.fit(X, y)
#Predict Output
predicted= model.predict(x_test)

also:
https://medium.com/analytics-vidhya/random-forest-a-model-designed-to-provide-structure-in-chaos-e267d559ca04

https://www.analyticsvidhya.com/blog/2018/10/interpret-random-forest-model-machine-learning-programmers/?utm_source=facebook.com


9) dimension reduction - pca

#Import Library
from sklearn import decomposition
#Assumed you have training and test data set as train and test
# Create PCA obeject pca= decomposition.PCA(n_components=k) #default value of k =min(n_sample, n_features)
# For Factor analysis
#fa= decomposition.FactorAnalysis()
# Reduced the dimension of training dataset using PCA
train_reduced = pca.fit_transform(train)
#Reduced the dimension of test dataset
test_reduced = pca.transform(test)
#For more detail on this, please refer  this link.

https://www.datacamp.com/courses/dimensionality-reduction-in-python

10) gradient boosting
GBM is a boosting algorithm used when we deal with plenty of data to make a prediction with high prediction power. 
Boosting is actually an ensemble of learning algorithms which combines the prediction of several base estimators in order to improve robustness over a single estimator. It combines multiple weak or average predictors to a build strong predictor. 
These boosting algorithms always work well in data science competitions like Kaggle, AV Hackathon, CrowdAnalytix.

#Import Library
from sklearn.ensemble import GradientBoostingClassifier
#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset
# Create Gradient Boosting Classifier object
model= GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0)
# Train the model using the training sets and check score
model.fit(X, y)
#Predict Output
predicted= model.predict(x_test)

11) XGBoost

from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
X = dataset[:,0:10]
Y = dataset[:,10:]
seed = 1

X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=seed)

model = XGBClassifier()

model.fit(X_train, y_train)

#Make predictions for test data
y_pred = model.predict(X_test)

12) Light GBM

data = np.random.rand(500, 10) # 500 entities, each contains 10 features
label = np.random.randint(2, size=500) # binary target

train_data = lgb.Dataset(data, label=label)
test_data = train_data.create_valid('test.svm')

param = {'num_leaves':31, 'num_trees':100, 'objective':'binary'}
param['metric'] = 'auc'

num_round = 10
bst = lgb.train(param, train_data, num_round, valid_sets=[test_data])

bst.save_model('model.txt')

# 7 entities, each contains 10 features
data = np.random.rand(7, 10)
ypred = bst.predict(data)

13) CatBoost
CatBoost is a recently open-sourced machine learning algorithm from Yandex. It can easily integrate with deep learning frameworks like Google’s TensorFlow and Apple’s Core ML.

The best part about CatBoost is that it does not require extensive data training like other ML models, and can work on a variety of data formats; not undermining how robust it can be.

Make sure you handle missing data well before you proceed with the implementation.

Catboost can automatically deal with categorical variables without showing the type conversion error, which helps you to focus on tuning your model better rather than sorting out trivial errors.

Learn more about Catboost from this article: https://www.analyticsvidhya.com/blog/2017/08/catboost-automated-categorical-data/

import pandas as pd
import numpy as np

from catboost import CatBoostRegressor

#Read training and testing files
train = pd.read_csv("train.csv")
test = pd.read_csv("test.csv")

#Imputing missing values for both train and test
train.fillna(-999, inplace=True)
test.fillna(-999,inplace=True)

#Creating a training set for modeling and validation set to check model performance
X = train.drop(['Item_Outlet_Sales'], axis=1)
y = train.Item_Outlet_Sales

from sklearn.model_selection import train_test_split

X_train, X_validation, y_train, y_validation = train_test_split(X, y, train_size=0.7, random_state=1234)
categorical_features_indices = np.where(X.dtypes != np.float)[0]

#importing library and building model
from catboost import CatBoostRegressormodel=CatBoostRegressor(iterations=50, depth=3, learning_rate=0.1, loss_function='RMSE')

model.fit(X_train, y_train,cat_features=categorical_features_indices,eval_set=(X_validation, y_validation),plot=True)

submission = pd.DataFrame()

submission['Item_Identifier'] = test['Item_Identifier']
submission['Outlet_Identifier'] = test['Outlet_Identifier']
submission['Item_Outlet_Sales'] = model.predict(test)

clustering:
https://www.analyticsvidhya.com/blog/2016/11/an-introduction-to-clustering-and-different-methods-of-clustering/?utm_source=facebook.com

-------------------------------
python excel:
https://www.datacamp.com/community/tutorials/python-excel-tutorial

datetime:
https://www.datacamp.com/community/tutorials/converting-strings-datetime-objects

customer segmentation:
https://www.datacamp.com/community/tutorials/introduction-customer-segmentation-python

feature selection:
https://www.datacamp.com/community/tutorials/feature-selection-python

featuretools:
https://www.analyticsvidhya.com/blog/2018/08/guide-automated-feature-engineering-featuretools-python/?utm_source=facebook.com


predicting churn:
https://www.datacamp.com/courses/hr-analytics-in-python-predicting-employee-churn

latent semantic modeling:
https://www.analyticsvidhya.com/blog/2018/10/stepwise-guide-topic-modeling-latent-semantic-analysis/?utm_source=facebook.com

web scraping - beautiful soup
https://medium.com/analytics-vidhya/web-scraping-wiki-tables-using-beautifulsoup-and-python-6b9ea26d8722

https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/?utm_source=facebook.com&utm_medium=social

https://www.datacamp.com/community/tutorials/web-scraping-using-python

https://www.datacamp.com/community/tutorials/tutorial-python-beautifulsoup-datacamp-tutorials

time series:
https://medium.com/analytics-vidhya/build-high-performance-time-series-models-using-auto-arima-in-python-and-r-f1d996583bf6

https://www.analyticsvidhya.com/blog/2018/02/time-series-forecasting-methods/?utm_source=facebook.com
preprocessing for time series:
https://medium.com/analytics-vidhya/preprocessing-for-time-series-forecasting-3a331dbfb9c2


ARIMA - depends on previous data point - bootstrapping methods don't preserve order
https://www.datacamp.com/courses/visualizing-time-series-data-in-python/

https://www.datacamp.com/courses/introduction-to-time-series-analysis-in-python

https://www.analyticsvidhya.com/blog/2018/09/multivariate-time-series-guide-forecasting-modeling-python-codes/?utm_source=facebook.com

https://medium.com/analytics-vidhya/encoding-time-series-as-images-b043becbdbf3

https://www.datacamp.com/community/tutorials/stocks-significance-testing-p-hacking

https://github.com/mythicalprogrammer/timeseries_meetup

recommendation engine:
https://www.analyticsvidhya.com/blog/2016/06/quick-guide-build-recommendation-engine-python/?utm_source=facebook.com

neural network:
https://medium.com/analytics-vidhya/build-your-first-neural-network-model-on-a-structured-dataset-using-keras-d9e7de5c6724

https://www.datacamp.com/community/blog/keras-cheat-sheet

https://medium.com/analytics-vidhya/neural-networks-for-digits-recognition-e11d9dff00d5

ensemble learning:
https://www.datacamp.com/community/tutorials/ensemble-learning-python

deep learning:
https://www.datacamp.com/courses/deep-learning-in-python?utm_source=fb_paid&utm_medium=cpc&utm_campaign=fb_ppa

https://www.analyticsvidhya.com/blog/2017/01/must-know-questions-deep-learning/?utm_source=facebook.com&utm_medium=social
multi armed bandit problem:
https://www.analyticsvidhya.com/blog/2018/09/reinforcement-multi-armed-bandit-scratch-python/?utm_source=facebook.com
da
genetic algorithm:
https://medium.com/analytics-vidhya/understanding-genetic-algorithms-in-the-artificial-intelligence-spectrum-7021b7cc25e7

LSTM stocks:
https://www.datacamp.com/community/tutorials/lstm-python-stock-market?%3Futm_source=fb_paid&utm_medium=cpc&utm_campaign=fb_ppa

data structures:
https://www.datacamp.com/community/tutorials/data-structures-python

data visualization:
https://www.datacamp.com/courses/introduction-to-data-visualization-with-python?utm_source=fb_paid&utm_medium=cpc&utm_campaign=dc_registered_visitors
https://www.analyticsvidhya.com/blog/2017/01/beginners-guide-to-create-beautiful-interactive-data-visualizations-using-plotly-in-r-and-python/?utm_source=facebook.com&utm_medium=social

graph theory:
https://www.analyticsvidhya.com/blog/2018/09/introduction-graph-theory-applications-python/?utm_source=facebook.com

https://www.analyticsvidhya.com/blog/2018/04/introduction-to-graph-theory-network-analysis-python-codes/?utm_source=facebook.com&utm_medium=social

network X:
https://www.datacamp.com/community/tutorials/networkx-python-graph-tutorial

jupyter notebooks:
https://www.analyticsvidhya.com/blog/2018/09/jupytext-lets-you-use-jupyter-notebooks-as-julia-python-and-r-scripts-or-markdown-documents/?utm_source=facebook.com

pca to auto encoder:
https://medium.com/analytics-vidhya/journey-from-principle-component-analysis-to-autoencoders-e60d066f191a

bias:
https://www.analyticsvidhya.com/blog/2018/09/ibm-open-sources-comprehensive-python-toolkit-for-detecting-fighting-bias-30-metrics-9-algorithms/?utm_source=facebook.com

databases:
https://medium.com/analytics-vidhya/programming-with-databases-in-python-using-sqlite-4cecbef51ab9

tensors w/ pytorch:
https://www.datacamp.com/community/tutorials/investigating-tensors-pytorch

tpot:
https://www.datacamp.com/community/tutorials/tpot-machine-learning-python

pickle:
https://www.datacamp.com/community/tutorials/pickle-python-tutorial

active learning:
https://www.datacamp.com/community/tutorials/active-learning

search engine marketing:
https://www.datacamp.com/community/tutorials/sem-data-science

finance example:
https://www.datacamp.com/courses/importing-managing-financial-data-in-python?utm_source=fb_paid&utm_medium=cpc&utm_campaign=fb_ppa 

https://www.datacamp.com/courses/intro-to-python-for-finance

probability distributions:
https://www.datacamp.com/community/tutorials/probability-distributions-python
https://www.analyticsvidhya.com/blog/2017/09/6-probability-distributions-data-science/?utm_source=facebook.com&utm_medium=social


geospatial:
https://www.datacamp.com/community/tutorials/geospatial-data-python
https://www.datacamp.com/courses/visualizing-geospatial-data-in-python

convert to apiusing flask:
https://www.datacamp.com/community/tutorials/machine-learning-models-api-python

facial recognition:
https://www.analyticsvidhya.com/blog/2018/08/a-simple-introduction-to-facial-recognition-with-python-codes/?utm_source=facebook.com

text:
https://www.analyticsvidhya.com/blog/2018/04/a-comprehensive-guide-to-understand-and-implement-text-classification-in-python/?utm_source=facebook.com

ML on GITHUB:
https://www.analyticsvidhya.com/blog/2018/11/best-machine-learning-github-repositories-reddit-threads-october-2018/?utm_source=facebook.com

JSON data:
https://www.datacamp.com/community/tutorials/json-data-python

monte carlo:
https://www.datacamp.com/community/tutorials/tutorial-monte-carlo

https://www.analyticsvidhya.com/blog/2018/11/reinforcement-learning-introduction-monte-carlo-learning-openai-gym/?utm_source=facebook.com

tidyverse:
https://www.datacamp.com/courses/modeling-with-data-in-the-tidyverse

python for data science:
https://www.datacamp.com/courses/intro-to-python-for-data-science?utm_source=fb_paid&utm_medium=cpc&utm_campaign=fb_ppa

sentiment analysis:
https://www.analyticsvidhya.com/blog/2018/07/hands-on-sentiment-analysis-dataset-python/?utm_source=facebook.com

https://courses.analyticsvidhya.com/courses/twitter-sentiment-analysis/?utm_source=facebook.com&utm_medium=social

shell commands for data science:
https://www.datacamp.com/courses/introduction-to-shell-for-data-science

ML box:
https://www.facebook.com/452065408218678/posts/1992788360813034/

hypothesis testing:
https://www.analyticsvidhya.com/blog/2015/09/hypothesis-testing-explained/?utm_source=facebook.com&utm_medium=social

fuzzy string logic:
https://www.datacamp.com/community/tutorials/fuzzy-string-python

outlier detetion:
https://www.analyticsvidhya.com/blog/2019/02/outlier-detection-python-pyod/?utm_source=facebook.com&utm_medium=social

face detection:
https://www.analyticsvidhya.com/blog/2018/12/introduction-face-detection-video-deep-learning-python/?utm_source=facebook.com&utm_medium=social

tensorflow - ML model in browser:
https://www.analyticsvidhya.com/blog/2019/06/build-machine-learning-model-in-your-browser-tensorflow-js-deeplearn-js/?utm_source=facebook.com&utm_medium=social
 
sequence modeling deep learning:
https://www.analyticsvidhya.com/blog/2019/01/sequence-models-deeplearning/?utm_source=facebook.com&utm_medium=social

understanding:
https://www.analyticsvidhya.com/blog/2019/08/decoding-black-box-step-by-step-guide-interpretable-machine-learning-models-python/?utm_source=facebook.com&utm_medium=social

